# Amazon-ml-challenge-2023

In order to clean a large dataset consisting of 2.2 million rows, we first used a memory-efficient approach of cleaning the data in chunks. The cleaning process involved removing non-English rows from the dataset by using the langdetect library to detect the language of the text in the 'TITLE' column. After cleaning the data in chunks, we concatenated the cleaned data into a single dataframe.
Next, we created a new column called 'TITLE' by combining the 'TITLE', 'BULLET_POINTS', and 'DESCRIPTION' columns of the dataset. This was done to create a unified text corpus to perform the tf-idf algorithm. We applied the tf-idf algorithm on the words in the 'TITLE' column, which gave us a matrix of weights corresponding to each word in the corpus. However, in cases where the testing data was unable to find a matching product ID, a range of product type IDs was defined to find the predicted length.
For the prediction task, we first matched the 'TITLE' of each testing data row with the 'TITLE' of each training data row using the product ID as a key. Once we found the matching 'TITLE' in the training dataset, we used the corresponding tf-idf matrix to predict the length of the testing data row.
Overall, this approach allowed us to efficiently clean and preprocess the large dataset, and perform accurate predictions using the tf-idf algorithm.
